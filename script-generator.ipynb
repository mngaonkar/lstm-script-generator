{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 50000\n",
    "INPUT_LENGTH = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles = json.loads(open('game-of-thrones-srt/season1.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines =  6658\n"
     ]
    }
   ],
   "source": [
    "s1 = []\n",
    "df = pd.read_json('game-of-thrones-srt/season1.json')\n",
    "\n",
    "for episode in range(len(df.columns)):\n",
    "    e = df[df.columns[episode]].dropna().sort_index()\n",
    "    s1 = s1 + list(e.values)\n",
    "\n",
    "print(\"total lines = \", len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[599, 65], [18, 21, 2, 796, 298, 710], [43, 543, 2166, 5, 1268, 51, 263, 543], [131, 2, 41, 10, 298, 2167, 544, 209, 3, 1060], [117, 79, 225, 512, 21, 5, 140, 58, 30], [4, 79, 225, 5, 140, 58, 30, 22, 168, 14, 8, 118], [60, 429, 61, 2, 80], [429, 40, 169, 54, 59, 33, 115, 219, 85, 3, 1, 155], [21, 1, 139, 711, 2], [87, 643, 84, 3, 1571, 1, 512]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(s1)\n",
    "seq = tokenizer.texts_to_sequences(s1)\n",
    "print(seq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length =  45036\n"
     ]
    }
   ],
   "source": [
    "corpus = [subitem for item in seq for subitem in item]\n",
    "print(\"corpus length = \", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size =  3943\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "print('vocab size = ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 20\n",
    "prediction_len = 1\n",
    "train_len = sentence_len - prediction_len\n",
    "\n",
    "train_seq = []\n",
    "for item in range(len(corpus) - sentence_len):\n",
    "    train_seq.append(corpus[item:item + sentence_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainy = []\n",
    "for i in train_seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size + 1, 50, input_length=train_len),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size - 1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 19, 50)            197200    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 19, 150)           120600    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3942)              595242    \n",
      "=================================================================\n",
      "Total params: 1,116,292\n",
      "Trainable params: 1,116,292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45016, 19)\n",
      "   45016\n",
      "0      1\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(trainX).shape)\n",
    "print(pd.get_dummies(np.asarray(trainy).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45016/45016 [==============================] - 69s 2ms/sample - loss: 6.4170 - acc: 0.0435\n",
      "Epoch 2/50\n",
      "45016/45016 [==============================] - 71s 2ms/sample - loss: 6.0567 - acc: 0.0504\n",
      "Epoch 3/50\n",
      "45016/45016 [==============================] - 70s 2ms/sample - loss: 5.9088 - acc: 0.0571\n",
      "Epoch 4/50\n",
      "45016/45016 [==============================] - 70s 2ms/sample - loss: 5.7966 - acc: 0.0579\n",
      "Epoch 5/50\n",
      "45016/45016 [==============================] - 71s 2ms/sample - loss: 5.6951 - acc: 0.0602\n",
      "Epoch 6/50\n",
      "45016/45016 [==============================] - 72s 2ms/sample - loss: 5.6062 - acc: 0.0638\n",
      "Epoch 7/50\n",
      "45016/45016 [==============================] - 74s 2ms/sample - loss: 5.5210 - acc: 0.0707\n",
      "Epoch 8/50\n",
      "45016/45016 [==============================] - 80s 2ms/sample - loss: 5.4253 - acc: 0.0805\n",
      "Epoch 9/50\n",
      "45016/45016 [==============================] - 78s 2ms/sample - loss: 5.3155 - acc: 0.0922\n",
      "Epoch 10/50\n",
      "45016/45016 [==============================] - 75s 2ms/sample - loss: 5.2123 - acc: 0.0996\n",
      "Epoch 11/50\n",
      "45016/45016 [==============================] - 85s 2ms/sample - loss: 5.1188 - acc: 0.1066\n",
      "Epoch 12/50\n",
      "45016/45016 [==============================] - 78s 2ms/sample - loss: 5.2043 - acc: 0.0983\n",
      "Epoch 13/50\n",
      "27584/45016 [=================>............] - ETA: 37s - loss: 5.0426 - acc: 0.1093"
     ]
    }
   ],
   "source": [
    "model.fit(np.asarray(trainX), pd.get_dummies(np.asarray(trainy)), batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_weights.hdf5')\n",
    "model.load_weights('model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "def generate_text(input_text, prediction_length):\n",
    "    tokens = tokenizer.texts_to_sequences([input_text])\n",
    "\n",
    "    while len(tokens[0]) < prediction_length:\n",
    "        if len(tokens[0]) <= INPUT_LENGTH:\n",
    "            padded_tokens = pad_sequences(tokens[-INPUT_LENGTH:], maxlen=INPUT_LENGTH)\n",
    "        else:\n",
    "            padded_tokens = [tokens[0][-INPUT_LENGTH:]]\n",
    "\n",
    "        prediction = model.predict(np.asarray(padded_tokens).reshape(1,-1))\n",
    "        tokens[0].append(prediction.argmax())\n",
    "        \n",
    "    tokens[0] = [134 if x==0 else x for x in tokens[0]]\n",
    "\n",
    "    generated_text = \" \".join(map(lambda x : token_to_word_map[x], tokens[0]))\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what do you know about nothing the gives are your brother and your talking feasting oh been in his sister leave the tongue to oh oh oh oh i rescue be understand have the orders when oh they in it it's they only will and name the please had oh\""
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"What do you know about warfare? - Nothing.\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"before me a things you you eddard in oh a if to so i our there's it it's i end i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo i drogo\""
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"kneel before me\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
